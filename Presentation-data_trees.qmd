---
title: "Presentation: Data Trees"
author: "Riley Sawyer"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: false
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---
### Introduction
Decision trees (DTs) are non-paremtreic supervised learning method used for classification and regression. This presentation will cover:
1. How decision trees work
2. The advanatages and disadvantages of decision trees
3. Classification
4. Regression
5. How to use decision trees in Python


### How decision trees work

**Structure of a decision tree**
A decision tree has a flow chart like structure that allows individuals to clearly visualize the decisioin making process. Decision trees have three main components:
1. **Root node**: This is the first node of the tree. All following nodes can be traced make to the root node.
2. **Internal nodes**: These nodes represent the features used for splitting the data. These features determine the path taken from this node onward. There are two types of internal nodes:
   - **Decision nodes**: nodes where a specific question, atribute, or choice is made to split the data.
   - **Chance nodes**: these nodes represent uncertainty in the decision-making process. Mulitple outcomes are possible here, so a probability is assigned to each outcome and the path is determined from there.
3. **Leaf nodes**: These are the terminal nodes of the tree that represent the final output or decision. In classification tasks, leaf nodes represent class labels, while in regression tasks, they represent continuous values.


![Decision Tree example](https://cdn.careerfoundry.com/en/wp-content/uploads/old-blog-uploads/decision-tree-example.jpg)


### Advantages and disadvantages of decision trees

**Advantages**
* Simple to understand and interpret
* Requires little data preprocessing, ei:
    + feature scaling
    + normalization
* Can handle both numerical and categorical data
* Can capture non-linear relationships
* Can be easily validated with statistical tests

**Disadvantages**
* Prone to overfitting, especially with deep trees. Can be remedied by:
    + Pruning
    + Setting a maximum depth
    + Using ensemble methods like random forests
* Can be unstable, as small changes in the data can lead to different tree structures
    + Can be remedied by ensemble methods like random forests
* Can be biased towards features with more levels. May not perform well with imbalanced datasets
* May not capture complex relationships as well as other models, such as random forests or gradient boosting machines, ei:
    + XOR problems
    + Parity problems
    + Mulitplexer problems

### Classification
Decision trees can be used to classify data into different categories. Ei,:
* Image classification
* Spam detection
* Customer segmentation

This uses `DecisionTreeClassifier` from `sklearn.tree` module in Python.

### Regression
Decision trees can also be used for regression tasks, where the goal is to predict a continuous value. Ei,:
* Predicting house prices
* Forecasting sales
* Estimating average customer spending

This uses `DecisionTreeRegressor` from `sklearn.tree` module in Python.


### How to use decision trees in Python
Decision trees can be implemented in Python using the `scikit-learn` library. However, here are other library you may want to use to supplement your DT usage.


**Step 1: Import necessary libraries**
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
```


**Step 2: Import scikit-learn modules**
Before accessing any decision tree model, you will need to import the designated library. Use the following code to install:
`pip install -U scikit-learn`

Now, you can use the following code to import the indicated modules:
```{python}
from sklearn import tree # this is where your DTs are located
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
```

`confusion_matrix` uses a table layout and is used to evaluate the performance of a classification model. It compares the predicted labels from your trained decision tree with the true labels from the data set.
* It is a summary of the correct and incorrect predicitons.

`accuracy_score` is a metric used to evaluate the performance of a classification model. It calculates the ratio of correctly predicted instances to the total number of instances in the dataset.

`classification_report` is a function that provides a detailed report of the precision, recall, F1-score, and support for each class in a classification model. It helps to evaluate the performance of the model for each class.
* precision: number of true positive predictions vs. number of positive predictions made by the model.
* recall: number of true positive predictions vs. number of actual positive instances in the dataset.
* F1-score: a combination of both precision and recall, an average of the two.

`train_test_split` is a function used to split a dataset into training and testing sets. It allows you to decide a random state for reproducibility.

`plot_tree` is a function used to visualize a decision tree model. Can help to understand how the model makes predictions and to identify important features in the dataset.


**Step 3: Name you decision tree model**
You'll know based on your statisitcal questions what type of decision tree you will want to call.

For classification:
```{python}
clf = tree.DecisionTreeClassifier()
```

For regression:
```{python}
reg = tree.DecisionTreeRegressor()
```


**Step 4: load data**
For this example, we will be using the iris dataset. It contains 150 samples of iris flowers, with 4 features (sepal length, sepal width, petal length, petal width) and a target variable (species of iris).

```{python}
from sklearn.datasets import load_iris
iris = load_iris()
```

Assing your features to `X` and `y`. Your data will always be `X` and your target variable will always be `y`.
```{python}
X = iris.data
y = iris.target
```

Always split your data into training and testing sets. If you skip this step, you will not be able to evaluate the performance of your model.
```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```
* `train_test_split` splits the data into training and testing sets. 
* `test_size` indicates how much of the dataset you want in your testing group (here, it is 20%).
* `random_state` ensures reproducibility. This can be any integer.


**Step 5: Fit your model**
Also knows as "training" your model. Fit your model only on the training set.

```{python}
clf.fit(X_train, y_train)
```
* `fit` is used to train a model on a training data. It takes a format as follows:
`your_model_type.fit(X_train, y_train)`


**Step 6: Make predictions**
Now that your model is trained, you can use it to predict the classifications from the testing set. Save the predictions that your model makes in a variable you can call upon later. Here, we will call it `y_pred`.

```{python}
y_pred = clf.predict(X_test)
```
* `predict` is used to make predictions on new data. It takes a format as follows:
`your_model_type.predict(X_test)`

**Step 7: Evaluate your model**
It is in good practice to test the performance of your model.

```{python}
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

You can also show the decision tree itself.
```{python}
plt.figure(figsize=(12,8))
plot_tree(clf,
        feature_names=iris.feature_names, # refers to x-variables
        class_names=iris.target_names,    #refers to y-variables
        filled=True)    # colors the nodes to indicate the majority class
plt.show()

